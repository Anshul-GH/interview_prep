{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello World.', 'Its a great day today.', 'I am luvin it!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code #1: Sentence Tokenization – Splitting sentences in the paragraph\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "  \n",
    "text = \"Hello World. Its a great day today. I am luvin it!\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello World.', 'Its a great day today.', 'I am luvin it!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code #2: PunktSentenceTokenizer – When we have huge chunks of data then it is efficient to use it.\n",
    "\n",
    "import nltk.data\n",
    "  \n",
    "# Loading PunktSentenceTokenizer using English pickle file\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "  \n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola amigo.', 'Estoy bien.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code #3: Tokenize sentence of different language – One can also tokenize sentence from different languages using different pickle file other than English.\n",
    "import nltk.data\n",
    "  \n",
    "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')\n",
    "  \n",
    "text = 'Hola amigo. Estoy bien.'\n",
    "spanish_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'World',\n",
       " 'is',\n",
       " 'such',\n",
       " 'a',\n",
       " 'cliche',\n",
       " '.',\n",
       " 'Lets',\n",
       " 'say',\n",
       " 'wassup',\n",
       " 'humans',\n",
       " '?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code #4: Word Tokenization – Splitting words in a sentence.\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "text = \"Hello World is such a cliche. Lets say wassup humans?\"\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc384f7e1c802d762e8ae2d35493986cf4d8dfda21fbfcf3845d3480326ba56e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
