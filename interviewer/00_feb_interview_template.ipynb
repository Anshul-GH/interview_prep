{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3\n",
    "# SQL Queries (Raw - advance level)\n",
    "# Strong understanding of Data Structures, Algorithms, and Object-Oriented Design. \n",
    "# ElasticSearch/MongoDB\n",
    "# Experience in developing Spark applications in Python (PySpark). \n",
    "# In-depth understanding of Data Warehouse/ODS, ETL concept, and modeling structure principles\n",
    "# AWS (Exposure to services like AWS Glue, EMR, Redshift)\n",
    "# Good to have familiarity with data visualization tools (Tableau/Power BI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3\n",
    "# Use the questions from coding and core files\n",
    "\n",
    "# Python Coding\n",
    "\n",
    "\n",
    "## BASIC\n",
    "\n",
    "# Given a list:\n",
    "lst = [1,2,3,4,6,7,8,9,12,4,2,5,7,2,5,2,1,4,6,3]\n",
    "# Push all the even numbers to the front and odd numbers to the back\n",
    "# Expected Output (one possibility):\n",
    "output = [6,8,4,2,4,2,12,2,4,2,6,7,1,5,3,9,7,3,5,1]\n",
    "# Note:\n",
    "# 1. The order of occurrence of the numbers can change\n",
    "# 2. Do not use any temporary variables.\n",
    "\n",
    "#############################################\n",
    "\n",
    "# Write a code to merge two lists iteratively in such a way that:\n",
    "# * For each iteration, one element will be picked up from each list and added to the output list\n",
    "# * After each iteration, the result in the output list should always be sorted\n",
    "# The lists can be of different lengths so if one gets exhausted, continue the iteration with the remaining elements of the other list\n",
    "list1 = [11, 4, 88, 66]\n",
    "list2 = [90, 55, 66, 33, 22, 99]\n",
    "# Example:\n",
    "# Iteration1: [11,90]\n",
    "# Iteration2: [4,11,55,90]\n",
    "# Iteration3: [4,11,55,66,88,90]\n",
    "# Iteration4: [4,11,33,55,66,66,88,90]\n",
    "# Iteration5: [4,11,22,33,55,66,66,88,90]\n",
    "# Iteration6: [4,11,22,33,55,66,66,88,90,99]\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "## INTER/ADVNC\n",
    "\n",
    "# Write a function to find the longest common prefix string amongst an array of strings.\n",
    "# If there is no common prefix, return an empty string \"\".\n",
    "\n",
    "# Example 1:\n",
    "# Input: strs = [\"flower\",\"flow\",\"flight\"]\n",
    "# Output: \"fl\"\n",
    "\n",
    "# Example 2:\n",
    "# Input: strs = [\"dog\",\"racecar\",\"car\"]\n",
    "# Output: \"\"\n",
    "# Explanation: There is no common prefix among the input strings.\n",
    "\n",
    "# Example 3:\n",
    "# Input: strs = [\"vet\",\"veteran\",\"vetter\"]\n",
    "# Output: \"vet\"\n",
    "\n",
    "# Example 4:\n",
    "# Input: strs = [\"apple\",\"crapple\",\"grapple\"]\n",
    "# Output: \"\"\n",
    "\n",
    "# Constraints:\n",
    "#     1 <= strs.length <= 200\n",
    "#     0 <= strs[i].length <= 200\n",
    "#     strs[i] consists of only lower-case English letters.\n",
    "\n",
    "#############################################\n",
    "\n",
    "# Fibonacci series using dynamic programming\n",
    "\n",
    "#############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Core\n",
    "\n",
    "# What will be the output:\n",
    "sample = 'hello',\n",
    "print(type(sample))\n",
    "\n",
    "# What will be the output:\n",
    "t2 = ([1,2,3,4], [5,6,7,8])\n",
    "t2[0].append(10)\n",
    "print(t2)\n",
    "\n",
    "# What will be the output:\n",
    "a = set('abracadabra')\n",
    "print(a)\n",
    "\n",
    "# Given a list, use list slicing to get the expected output:\n",
    "lst = [1,2,3,4,5]\n",
    "\n",
    "# Expected Output\n",
    "# [5,3,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL\n",
    "\n",
    "#1.\n",
    "# EmployeeInfo Table: \n",
    "EmpID\n",
    "EmpFname\n",
    "EmpLname\n",
    "Department\n",
    "Project\n",
    "Address\n",
    "DOB\n",
    "Gender\n",
    "\n",
    "Write the SQL qyery to fetch the department-wise count of \n",
    "employees sorted by department’s count in ascending order.\n",
    "\n",
    "Query:\n",
    "SELECT Department, count(EmpID) AS EmpDeptCount \n",
    "FROM EmployeeInfo GROUP BY Department \n",
    "ORDER BY EmpDeptCount ASC;\n",
    "\n",
    "# ##################################################################\n",
    "\n",
    "#2.\n",
    "# Table1 and Table2:\n",
    "TABLE_1     TABLE_2\n",
    "COLUMN_1    COLUMN_2\n",
    "1           0\n",
    "1           1\n",
    "0           1\n",
    "0           2\n",
    "2           4\n",
    "3           null\n",
    "null        null\n",
    "null        null\n",
    "\n",
    "What will be the output of below query:\n",
    "\n",
    "Query:\n",
    "SELECT count(*)\n",
    "FROM TABLE_1 LEFT JOIN TABLE_2\n",
    "ON TABLE_1.COLUMN_1 = TABLE_2.COLUMN_2;\n",
    "\n",
    "\n",
    "#3. Find duplicates in a table:\n",
    "Select * from Employee a where rowid <>( select max(rowid) from Employee b where a.Employee_num=b.Employee_num);\n",
    "\n",
    "#4. Find second highest salary:\n",
    "Select distinct Salary from Employee e1 where 2=Select count(distinct Salary) from Employee e2 where e1.salary<=e2.salary;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Search\n",
    "\n",
    "1. What is a document in Elastic Search?\n",
    "In an Elastic search, a document is a basic unit of information that can be indexed. It is expressed in JSON (key: value) pair. ‘{“user”: “nullcon”}’. Every single Document is associated with a type and a unique id.\n",
    "\n",
    "2. Define the Term Shard\n",
    "Every index can be split into several shards to be able to distribute data. The shard is the atomic part of an index, which can be distributed over the cluster if you want to add more nodes.\n",
    "\n",
    "# ####################################################\n",
    "\n",
    "# Mongo DB\n",
    "\n",
    "1. What is a Document in MongoDB?\n",
    "A Document in MongoDB is an ordered set of keys with associated values. It is represented by a map, hash, or dictionary. In JavaScript, documents are represented as objects:\n",
    "{\"greeting\" : \"Hello world!\"}\n",
    "Complex documents will contain multiple key/value pairs:\n",
    "{\"greeting\" : \"Hello world!\", \"views\" : 3}\n",
    "\n",
    "2. What is a Collection in MongoDB?\n",
    "A collection in MongoDB is a group of documents. If a document is the MongoDB analog of a row in a relational database, then a collection can be thought of as the analog to a table.\n",
    "Documents within a single collection can have any number of different “shapes.”, i.e. collections have dynamic schemas. \n",
    "For example, both of the following documents could be stored in a single collection:\n",
    "{\"greeting\" : \"Hello world!\", \"views\": 3}\n",
    "{\"signoff\": \"Good bye\"}\n",
    "\n",
    "3. What are some features of MongoDB?\n",
    "\n",
    "    Indexing: It supports generic secondary indexes and provides unique, compound, geospatial, and full-text indexing capabilities as well.\n",
    "    Aggregation: It provides an aggregation framework based on the concept of data processing pipelines.\n",
    "    Special collection and index types: It supports time-to-live (TTL) collections for data that should expire at a certain time\n",
    "    File storage: It supports an easy-to-use protocol for storing large files and file metadata.\n",
    "    Sharding: Sharding is the process of splitting data up across machines.\n",
    "\n",
    "4. Explain the process of Sharding.\n",
    "\n",
    "Sharding is the process of splitting data up across machines. We also use the term “partitioning” sometimes to describe this concept. We can store more data and handle more load without requiring larger or more powerful machines, by putting a subset of data on each machine.\n",
    "In the figure below, RS0 and RS1 are shards. MongoDB’s sharding allows you to create a cluster of many machines (shards) and break up a collection across them, putting a subset of data on each shard. This allows your application to grow beyond the resource limits of a standalone server or replica set.\n",
    "\n",
    "# ####################################################\n",
    "\n",
    "# PySpark\n",
    "\n",
    "1. What's the difference between an RDD, a DataFrame, and a DataSet?\n",
    "RDD-\n",
    "    It is Spark's structural square. RDDs contain all datasets and dataframes.\n",
    "    If a similar arrangement of data needs to be calculated again, RDDs can be efficiently reserved.\n",
    "    It's useful when you need to do low-level transformations, operations, and control on a dataset.\n",
    "    It's more commonly used to alter data with functional programming structures than with domain-specific expressions.\n",
    "\n",
    "DataFrame-\n",
    "    It allows the structure, i.e., lines and segments, to be seen. You can think of it as a database table.\n",
    "    Optimized Execution Plan- The catalyst analyzer is used to create query plans.\n",
    "    One of the limitations of dataframes is Compile Time Wellbeing, i.e., when the structure of information is unknown, no control of information is possible.\n",
    "    Also, if you're working on Python, start with DataFrames and then switch to RDDs if you need more flexibility.\n",
    "\n",
    "DataSet (A subset of DataFrames)-\n",
    "    It has the best encoding component and, unlike information edges, it enables time security in an organized manner.\n",
    "    If you want a greater level of type safety at compile-time, or if you want typed JVM objects, Dataset is the way to go.\n",
    "    Also, you can leverage datasets in situations where you are looking for a chance to take advantage of Catalyst optimization or even when you are trying to benefit from Tungsten’s fast code generation.\n",
    "\n",
    "\n",
    "2. What are the different ways to handle row duplication in a PySpark DataFrame?\n",
    "There are two ways to handle row duplication in PySpark dataframes. The distinct() function in PySpark is used to drop/remove duplicate rows (all columns) from a DataFrame, while dropDuplicates() is used to drop rows based on one or more columns. \n",
    "\n",
    "# ####################################################\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
