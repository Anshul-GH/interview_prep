1. What is tokenization?
Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.

2. Python nltk library?
I am looking to tokenize a text in Spanish.
How to do that?
<!-- 
import nltk.data
  
spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')
  
text = 'Hola amigo. Estoy bien.'
spanish_tokenizer.tokenize(text)
-->
<!-- 
from nltk.tokenize import word_tokenize
  
text = "Hello everyone. Welcome to GeeksforGeeks."
word_tokenize(text)
-->

